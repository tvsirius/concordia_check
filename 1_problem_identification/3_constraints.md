# Constraints

Doing this proposed simulation seems to fit into the scope the constraints I have. I may face such troubles:
1. Finding enough real life simulation, I could model - in this case I believe I could find at least one.
2. Time or Money constraints for proper simulation. ChatCPT that will obviously used in this case needs some time to process a request, and it is hard to say how many thousands of requests we will need for simulation. It also charges you with some money for each toked. 
Based on my previous simulation of ChatGPT Mental Support: I run 200 requests: request for topic determination, 3 request for getting response and from 10 to 3 request to compare response with human responses. It costs me about $2 so I believe it wound not be a problem. 
Alternative approaches:
- [Ragdoll-studio](https://ragdoll-studio.vercel.app/dolls#) also offers the solution to this issue.
- Try to delpoy Llama 3 (try even Llama 2) locally using llama.cpp (for running big LLM on limited local GPU RAM)

3. Interpreting the results. A support of a domain expert may be needed. 
